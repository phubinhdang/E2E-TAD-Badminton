{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filter by different prediction scores and merge the overlapping rally predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Iterable\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds_by_method = {\n",
    "    \"Isodata\": 0.3329599041272786,\n",
    "    \"Li\": 0.14066436881847458,\n",
    "    \"Mean\": 0.03914363852704741,\n",
    "    \"Minimum\": 0.24126479546077917,\n",
    "    \"Otsu\": 0.32968507881776077,\n",
    "    \"Triangle\": 0.0316759756516376,\n",
    "    \"Yen\": 0.07097387936585164,\n",
    "    \"prediction score = \": 0.9,\n",
    "}\n",
    "\n",
    "# thresholds_by_method = {\n",
    "#     \"prediction score\": 0.6,\n",
    "#     \"prediction score 1\": 0.65,\n",
    "#     \"prediction score 2\": 0.7,\n",
    "#     \"prediction score 3\": 0.75,\n",
    "#     \"prediction score 4\": 0.8,\n",
    "#     \"prediction score 5\": 0.85,\n",
    "#     \"prediction score 6\": 0.9,\n",
    "#     \"prediction score 7\": 0.95,\n",
    "# }\n",
    "\n",
    "score_thresholds = list(thresholds_by_method.values())\n",
    "methods = list(thresholds_by_method.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_by_method = []\n",
    "match_name = \"ginting_axelsen\"\n",
    "source_video_fps = 25\n",
    "with open(f'../data/output/badminton/{match_name}_detection_raw.json') as f:\n",
    "    d = json.load(f)    \n",
    "\n",
    "def ss_to_hhmmss(seconds):\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    seconds = int(seconds % 60)\n",
    "    return f\"{hours:02}:{minutes:02}:{seconds:02}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in score_thresholds:\n",
    "    pred_segs = []\n",
    "    segs = d[\"results\"][match_name]\n",
    "    for seg in segs:\n",
    "        pred_segs.append({\"start\": seg[\"segment\"][0], \"end\": seg[\"segment\"][1], \"score\": seg[\"score\"]})\n",
    "    df = pd.DataFrame(pred_segs)\n",
    "    # filter\n",
    "    df = df[df['score'] > t]\n",
    "    df = df.reset_index()\n",
    "    df = df.assign(start_hhmmss=df['start'].apply(ss_to_hhmmss))\n",
    "    df = df.assign(end_hhmmss=df['end'].apply(ss_to_hhmmss))\n",
    "    # merge\n",
    "    merged_intervals = []\n",
    "\n",
    "    # Start with the first interval\n",
    "    current_start = df['start'].values[0]\n",
    "    current_end = df['end'].values[0]\n",
    "\n",
    "    # Iterate through the DataFrame rows\n",
    "    for i in range(1, len(df)):\n",
    "        row_start = df.loc[i, 'start']\n",
    "        row_end = df.loc[i, 'end']\n",
    "\n",
    "        # If the current interval overlaps with the previous one, merge them\n",
    "        if row_start <= current_end:\n",
    "            current_end = max(current_end, row_end)  # Update the end of the merged interval\n",
    "        else:\n",
    "            # No overlap, add the previous interval and start a new one\n",
    "            merged_intervals.append([current_start, current_end])\n",
    "            current_start = row_start\n",
    "            current_end = row_end\n",
    "\n",
    "    # Append the last interval\n",
    "    merged_intervals.append([current_start, current_end])\n",
    "\n",
    "    # Create a new DataFrame from the merged intervals\n",
    "    merged_df = pd.DataFrame(merged_intervals, columns=['start', 'end'])\n",
    "    merged_df = merged_df.assign(start_hhmmss=merged_df['start'].apply(ss_to_hhmmss))\n",
    "    merged_df = merged_df.assign(end_hhmmss=merged_df['end'].apply(ss_to_hhmmss))\n",
    "    dfs_by_method.append(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get precision and recall for processed predictions by comparing with ground truth rallies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare each with the ground truth segments\n",
    "# TODO: remove duplicate code\n",
    "def ss_to_mmss(num_of_seconds) -> str:\n",
    "    \"\"\"Converts start_ss (in seconds) to minute:second format.\"\"\"\n",
    "    minutes = int(num_of_seconds // 60)  # Calculate minutes\n",
    "    seconds = int(num_of_seconds % 60)  # Calculate remaining seconds\n",
    "    return f\"{minutes:02}:{seconds:02}\"  # Format as mm:ss\n",
    "\n",
    "@dataclass\n",
    "class GroundTruthSegment:\n",
    "    start_ss: float\n",
    "    end_ss: float\n",
    "    start_mmss: str\n",
    "    end_mmss: str\n",
    "    is_matched: bool\n",
    "    score_board: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PredictionSegment:\n",
    "    start_ss: float\n",
    "    end_ss: float\n",
    "    start_mmss: str\n",
    "    end_mmss: str\n",
    "    best_iou: float\n",
    "    best_match_gt_segment: Optional[GroundTruthSegment]\n",
    "\n",
    "def get_gts() -> List[GroundTruthSegment]:\n",
    "    gts = []\n",
    "    df_gt = pd.read_csv('data/RallySeg_GT.csv')\n",
    "    df_gt['start_ss'] = df_gt['Start'] / source_video_fps\n",
    "    df_gt['end_ss'] = df_gt['End'] / source_video_fps\n",
    "    for s, e, score_board in zip(df_gt['start_ss'], df_gt['end_ss'], df_gt['Score']):\n",
    "        gts.append(GroundTruthSegment(s, e, ss_to_mmss(s), ss_to_mmss(e), False, score_board))\n",
    "    return gts\n",
    "\n",
    "def get_preds(df_preds: pd.DataFrame) -> List[PredictionSegment]:\n",
    "    pred_segments = []\n",
    "    for s, e in zip(df_preds['start'], df_preds['end']):\n",
    "        pred_segments.append(PredictionSegment(s, e, ss_to_mmss(s), ss_to_mmss(e), 0.0, None))\n",
    "    return pred_segments\n",
    "\n",
    "\n",
    "def find_best_iou(pred: PredictionSegment, gts: List[GroundTruthSegment]) -> (float, GroundTruthSegment):\n",
    "    best_iou = 0.0\n",
    "    best_match_gt_segment = None\n",
    "    for gt in gts:\n",
    "        iou = calc_iou(pred, gt)\n",
    "        if iou > best_iou:\n",
    "            best_iou = iou\n",
    "            best_match_gt_segment = gt\n",
    "    return best_iou, best_match_gt_segment\n",
    "\n",
    "def calc_iou(pred: PredictionSegment, gt: GroundTruthSegment) -> float:\n",
    "    inter_start = max(pred.start_ss, gt.start_ss)\n",
    "    inter_end = min(pred.end_ss, gt.end_ss)\n",
    "\n",
    "    # Calculate the length of the intersection\n",
    "    intersection = max(0, inter_end - inter_start)\n",
    "\n",
    "    # Calculate the start and end of the union\n",
    "    union_start = min(pred.start_ss, gt.start_ss)\n",
    "    union_end = max(pred.end_ss, gt.end_ss)\n",
    "\n",
    "    # Calculate the length of the union\n",
    "    union = union_end - union_start\n",
    "\n",
    "    # Calculate IoU\n",
    "    if union == 0:\n",
    "        return 0.0  # Handle case when both segments are points\n",
    "    iou = intersection / union\n",
    "    return iou\n",
    "\n",
    "def create_confusion_matrix(preds: List[PredictionSegment], gts: List[GroundTruthSegment], iou_threshold: float) -> (\n",
    "        float, float, float):\n",
    "    # assign iou to each pred segment\n",
    "    for pred in preds:\n",
    "        # which gt, for that the pred has the largest iou, the gt wil be then remove from gts\n",
    "        best_iou, best_gt_match = find_best_iou(pred, gts)\n",
    "        pred.best_iou = best_iou\n",
    "        if best_gt_match:\n",
    "            pred.best_match_gt_segment = dataclasses.replace(best_gt_match)\n",
    "            gts.remove(best_gt_match)\n",
    "    # count TP, FP, FN\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    for pred in preds:\n",
    "        # true positive\n",
    "        if pred.best_iou >= iou_threshold:\n",
    "            tp += 1\n",
    "        # false positive\n",
    "        else:\n",
    "            fp += 1\n",
    "    fn = len(gts)\n",
    "    # based on this tutorial: https://towardsdatascience.com/what-is-average-precision-in-object-detection-localization-algorithms-and-how-to-calculate-it-3f330efe697b\n",
    "    # fn does not include preds that have iou smaller than threshold_iou\n",
    "    # under_threshold = [p for p in preds if 0.0 < p.best_iou < iou_threshold]\n",
    "    # fn = fn + len(under_threshold)\n",
    "    return tp, fp, fn\n",
    "\n",
    "\n",
    "def calc_precision_and_recall(tp, fp, fn):\n",
    "    return tp / (tp + fp), tp / (tp + fn)\n",
    "\n",
    "def calc_precisions_and_recalls(thresholds, preds: List[PredictionSegment]) -> (List[float], List[float]):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    for i, t in enumerate(thresholds):\n",
    "        gts = get_gts()\n",
    "        if i == 0:\n",
    "            # for debug\n",
    "            print(f\"preds / gts len: {len(preds)}/{len(gts)}\")\n",
    "        tp, fp, fn = create_confusion_matrix(preds, gts, t)\n",
    "        precision, recall = calc_precision_and_recall(tp, fp, fn)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "    return precisions, recalls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw the precision and recall curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds_grundprojekt() -> List[PredictionSegment]:\n",
    "    df_preds = pd.read_csv('data/ginting_axelsen_grundprojekt.csv')\n",
    "    df_preds = df_preds[df_preds['pred_is_rally'] == 1]\n",
    "    df_preds = df_preds.assign(start_ss=(df_preds['start'] / source_video_fps).round(2))\n",
    "    df_preds = df_preds.assign(end_ss=(df_preds['end'] / source_video_fps).round(2))\n",
    "    raw_pred_segments = []\n",
    "    for s, e in zip(df_preds['start_ss'], df_preds['end_ss']):\n",
    "        raw_pred_segments.append(PredictionSegment(s, e, ss_to_mmss(s), ss_to_mmss(e), 0.0, None))\n",
    "    pred_segments = []\n",
    "    merged_segment = None\n",
    "    for i in range(len(raw_pred_segments) - 1):\n",
    "        current = raw_pred_segments[i]\n",
    "        next = raw_pred_segments[i + 1]\n",
    "        if not merged_segment:\n",
    "            merged_segment = dataclasses.replace(current)\n",
    "        if current.end_ss == next.start_ss:\n",
    "            merged_segment.end_ss = next.end_ss\n",
    "        else:\n",
    "            pred_segments.append(dataclasses.replace(merged_segment))\n",
    "            merged_segment = None\n",
    "     # the last predicted rally\n",
    "    pred_segments.append(dataclasses.replace(merged_segment))\n",
    "    return pred_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_curves(curves_data: List[List[float]],\n",
    "                thresholds: Iterable[float],\n",
    "                legends: List[str], \n",
    "                score_thresholds: List[float],\n",
    "                y_label: str,\n",
    "                title: str):\n",
    "    colors = itertools.cycle(plt.cm.tab10.colors)  # Use tab10 colormap for variety of colors\n",
    "\n",
    "    # sort by the first precision\n",
    "    sorted_data = sorted(zip([precisions[0] for precisions in curves_data], curves_data, legends, score_thresholds))\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Iterate over each curve in sorted order\n",
    "    for first_precision, precisions, legend, score_threshold in sorted_data:\n",
    "        color = next(colors)\n",
    "        label = legend if score_threshold is None else f\"{legend} ({score_threshold:.2f})\"\n",
    "        plt.plot(thresholds, precisions, marker='o', color=color, label=label)  \n",
    "\n",
    "    # Create a legend with the original order of legends\n",
    "    plt.legend(loc='best')  # Show legend in the best location\n",
    "\n",
    "    plt.xlabel(\"IoU thresholds\")\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw precision curves\n",
    "thresholds_iou = np.arange(start=0.5, stop=0.91, step=0.1)\n",
    "precisions_by_mehod = []\n",
    "recalls_by_method = []\n",
    "for df, method in zip(dfs_by_method, methods):\n",
    "    print(f\"method: {method}\")\n",
    "    precisions, recalls = calc_precisions_and_recalls(thresholds_iou, get_preds(df))\n",
    "    precisions_by_mehod.append(precisions)\n",
    "    recalls_by_method.append(recalls)\n",
    "\n",
    "# grundprojekt results\n",
    "precisions, recalls = calc_precisions_and_recalls(thresholds_iou, get_preds_grundprojekt())\n",
    "precisions_by_mehod.append(precisions)\n",
    "recalls_by_method.append(recalls)\n",
    "methods.append(\"Grundprojekt\")\n",
    "score_thresholds.append(None)\n",
    "\n",
    "title = \"Precision by prediction score threshold\"\n",
    "draw_curves(curves_data=precisions_by_mehod,\n",
    "            thresholds=thresholds_iou,\n",
    "            legends=methods,\n",
    "            score_thresholds=score_thresholds,\n",
    "            y_label=\"Precision\",\n",
    "            title=title,\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw recall curves\n",
    "title = \"Recall by prediction score threshold\"\n",
    "draw_curves(curves_data=recalls_by_method,\n",
    "            thresholds=thresholds_iou,\n",
    "            legends=methods,\n",
    "            score_thresholds=score_thresholds,\n",
    "            y_label=\"Recall\",\n",
    "            title=title,\n",
    "            )\n",
    "for method, recall in zip(methods, recalls_by_method):\n",
    "    print(f\"{method}: {recall[-1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e2etad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
